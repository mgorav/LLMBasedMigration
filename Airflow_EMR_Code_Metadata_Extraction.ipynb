{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0c17d98",
   "metadata": {},
   "source": [
    "# Airflow and EMR Code Metadata Extraction Notebook\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This notebook is designed to analyze Airflow and EMR code to extract important metadata such as storage details on S3, data ownership, and other relevant information. It utilizes advanced Language Models (LLMs) to understand and extract information from the code.\n",
    "\n",
    "## Objective\n",
    "\n",
    "The goal is to provide an automated method to read and interpret Airflow and EMR code, identifying key metadata for better data management and governance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d20cfec",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Install Required Libraries\n",
    "\n",
    "The notebook requires specific libraries to interface with the LLM models and to handle code analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install boto3  # For AWS S3 interactions\n",
    "!pip install apache-airflow  # For Airflow code interpretation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e17bf6eae93eaa85"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Choosing the Model\n",
    "\n",
    "We will use a model from the Hugging Face `transformers` library that is best suited for understanding and analyzing code. GPT-Neo or GPT-3 could be ideal choices for this task, owing to their understanding of natural language and code.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37d26282d0238353"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "# Initialize the model\n",
    "model = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')  # Example model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:07:11.035514Z",
     "start_time": "2024-01-24T12:06:54.434708Z"
    }
   },
   "id": "ab2c76ed5302cbec",
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Code Analysis Function\n",
    "\n",
    "### Defining the Analysis Function\n",
    "The function below is designed to read the Airflow and EMR code, use the LLM to extract relevant metadata like S3 storage details, data owner, etc.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d59d61127d7044a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def extract_metadata_from_code(code):\n",
    "    \"\"\"Extract metadata from Airflow/EMR code using LLM.\"\"\"\n",
    "    prompt = 'Extract metadata ONLY NO CODE OUTPUT, such as S3 storage details and data owner from the following code:\\n' + code\n",
    "    metadata = model(prompt)[0]['generated_text']\n",
    "    return metadata\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:14:17.179445Z",
     "start_time": "2024-01-24T12:14:17.177130Z"
    }
   },
   "id": "5291ae4845960dd0",
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Input Code\n",
    "\n",
    "Input the Airflow or EMR code in the cell below to extract metadata.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "881266f242919e79"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Code:\n",
      " \n",
      "from airflow import DAG\n",
      "from airflow.operators.python_operator import PythonOperator\n",
      "from airflow.providers.amazon.aws.operators.emr_create_job_flow import EmrCreateJobFlowOperator\n",
      "from airflow.providers.amazon.aws.sensors.emr_job_flow import EmrJobFlowSensor\n",
      "from datetime import datetime, timedelta\n",
      "import boto3\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'data_team',\n",
      "    'depends_on_past': False,\n",
      "    'start_date': datetime(2021, 1, 1),\n",
      "    'email': ['example@example.com'],\n",
      "    'email_on_failure': True,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 3,\n",
      "    'retry_delay': timedelta(minutes=10),\n",
      "}\n",
      "\n",
      "dag = DAG('complex_emr_dag',\n",
      "          default_args=default_args,\n",
      "          description='A complex DAG to process data using EMR',\n",
      "          schedule_interval=timedelta(days=1),\n",
      "          catchup=False)\n",
      "\n",
      "def preprocess_data():\n",
      "    # Code to preprocess data\n",
      "    print(\"Data Preprocessing\")\n",
      "    # Simulated interaction with S3\n",
      "    s3 = boto3.client('s3')\n",
      "    bucket_name = 'mydata-bucket'\n",
      "    s3.upload_file('localfile.txt', bucket_name, 'processed/processedfile.txt')\n",
      "\n",
      "preprocess_task = PythonOperator(task_id='preprocess_data',\n",
      "                                 python_callable=preprocess_data,\n",
      "                                 dag=dag)\n",
      "\n",
      "# EMR Job Flow configuration\n",
      "job_flow_overrides = {\n",
      "    'Name': 'MyEMRCluster',\n",
      "    # Add more configuration options as required\n",
      "}\n",
      "\n",
      "create_emr_cluster = EmrCreateJobFlowOperator(\n",
      "    task_id='create_emr_cluster',\n",
      "    job_flow_overrides=job_flow_overrides,\n",
      "    aws_conn_id='aws_default',\n",
      "    emr_conn_id='emr_default',\n",
      "    dag=dag)\n",
      "\n",
      "check_emr_cluster = EmrJobFlowSensor(\n",
      "    task_id='check_emr_cluster',\n",
      "    job_flow_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}\",\n",
      "    aws_conn_id='aws_default',\n",
      "    dag=dag)\n",
      "\n",
      "def analyze_data():\n",
      "    # Code to analyze data\n",
      "    print(\"Data Analysis\")\n",
      "    # Simulated error\n",
      "    raise Exception(\"Simulated error during data analysis\")\n",
      "\n",
      "analyze_data_task = PythonOperator(task_id='analyze_data',\n",
      "                                   python_callable=analyze_data,\n",
      "                                   retries=2,\n",
      "                                   dag=dag)\n",
      "\n",
      "preprocess_task >> create_emr_cluster >> check_emr_cluster >> analyze_data_task\n"
     ]
    }
   ],
   "source": [
    "# Sample Airflow/EMR code\n",
    "code = '''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python_operator import PythonOperator\n",
    "from airflow.providers.amazon.aws.operators.emr_create_job_flow import EmrCreateJobFlowOperator\n",
    "from airflow.providers.amazon.aws.sensors.emr_job_flow import EmrJobFlowSensor\n",
    "from datetime import datetime, timedelta\n",
    "import boto3\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data_team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2021, 1, 1),\n",
    "    'email': ['example@example.com'],\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=10),\n",
    "}\n",
    "\n",
    "dag = DAG('complex_emr_dag',\n",
    "          default_args=default_args,\n",
    "          description='A complex DAG to process data using EMR',\n",
    "          schedule_interval=timedelta(days=1),\n",
    "          catchup=False)\n",
    "\n",
    "def preprocess_data():\n",
    "    # Code to preprocess data\n",
    "    print(\"Data Preprocessing\")\n",
    "    # Simulated interaction with S3\n",
    "    s3 = boto3.client('s3')\n",
    "    bucket_name = 'mydata-bucket'\n",
    "    s3.upload_file('localfile.txt', bucket_name, 'processed/processedfile.txt')\n",
    "\n",
    "preprocess_task = PythonOperator(task_id='preprocess_data',\n",
    "                                 python_callable=preprocess_data,\n",
    "                                 dag=dag)\n",
    "\n",
    "# EMR Job Flow configuration\n",
    "job_flow_overrides = {\n",
    "    'Name': 'MyEMRCluster',\n",
    "    # Add more configuration options as required\n",
    "}\n",
    "\n",
    "create_emr_cluster = EmrCreateJobFlowOperator(\n",
    "    task_id='create_emr_cluster',\n",
    "    job_flow_overrides=job_flow_overrides,\n",
    "    aws_conn_id='aws_default',\n",
    "    emr_conn_id='emr_default',\n",
    "    dag=dag)\n",
    "\n",
    "check_emr_cluster = EmrJobFlowSensor(\n",
    "    task_id='check_emr_cluster',\n",
    "    job_flow_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}\",\n",
    "    aws_conn_id='aws_default',\n",
    "    dag=dag)\n",
    "\n",
    "def analyze_data():\n",
    "    # Code to analyze data\n",
    "    print(\"Data Analysis\")\n",
    "    # Simulated error\n",
    "    raise Exception(\"Simulated error during data analysis\")\n",
    "\n",
    "analyze_data_task = PythonOperator(task_id='analyze_data',\n",
    "                                   python_callable=analyze_data,\n",
    "                                   retries=2,\n",
    "                                   dag=dag)\n",
    "\n",
    "preprocess_task >> create_emr_cluster >> check_emr_cluster >> analyze_data_task\n",
    "'''\n",
    "print('Input Code:\\n', code)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:14:18.519776Z",
     "start_time": "2024-01-24T12:14:18.512004Z"
    }
   },
   "id": "9d1ba66af547f7e9",
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Extract and Display Metadata\n",
    "\n",
    "Run the cell below to extract and display the metadata from the input code.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6a93074eb2f12461"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/Users/gauravmalhotra/lib/python3.11/site-packages/transformers/generation/utils.py:1136: UserWarning: Input length of input_ids is 996, but `max_length` is set to 50. This can lead to unexpected behavior. You should consider increasing `max_new_tokens`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Metadata:\n",
      " Extract metadata ONLY NO CODE OUTPUT, such as S3 storage details and data owner from the following code:\n",
      "\n",
      "from airflow import DAG\n",
      "from airflow.operators.python_operator import PythonOperator\n",
      "from airflow.providers.amazon.aws.operators.emr_create_job_flow import EmrCreateJobFlowOperator\n",
      "from airflow.providers.amazon.aws.sensors.emr_job_flow import EmrJobFlowSensor\n",
      "from datetime import datetime, timedelta\n",
      "import boto3\n",
      "\n",
      "default_args = {\n",
      "    'owner': 'data_team',\n",
      "    'depends_on_past': False,\n",
      "    'start_date': datetime(2021, 1, 1),\n",
      "    'email': ['example@example.com'],\n",
      "    'email_on_failure': True,\n",
      "    'email_on_retry': False,\n",
      "    'retries': 3,\n",
      "    'retry_delay': timedelta(minutes=10),\n",
      "}\n",
      "\n",
      "dag = DAG('complex_emr_dag',\n",
      "          default_args=default_args,\n",
      "          description='A complex DAG to process data using EMR',\n",
      "          schedule_interval=timedelta(days=1),\n",
      "          catchup=False)\n",
      "\n",
      "def preprocess_data():\n",
      "    # Code to preprocess data\n",
      "    print(\"Data Preprocessing\")\n",
      "    # Simulated interaction with S3\n",
      "    s3 = boto3.client('s3')\n",
      "    bucket_name = 'mydata-bucket'\n",
      "    s3.upload_file('localfile.txt', bucket_name, 'processed/processedfile.txt')\n",
      "\n",
      "preprocess_task = PythonOperator(task_id='preprocess_data',\n",
      "                                 python_callable=preprocess_data,\n",
      "                                 dag=dag)\n",
      "\n",
      "# EMR Job Flow configuration\n",
      "job_flow_overrides = {\n",
      "    'Name': 'MyEMRCluster',\n",
      "    # Add more configuration options as required\n",
      "}\n",
      "\n",
      "create_emr_cluster = EmrCreateJobFlowOperator(\n",
      "    task_id='create_emr_cluster',\n",
      "    job_flow_overrides=job_flow_overrides,\n",
      "    aws_conn_id='aws_default',\n",
      "    emr_conn_id='emr_default',\n",
      "    dag=dag)\n",
      "\n",
      "check_emr_cluster = EmrJobFlowSensor(\n",
      "    task_id='check_emr_cluster',\n",
      "    job_flow_id=\"{{ task_instance.xcom_pull(task_ids='create_emr_cluster', key='return_value') }}\",\n",
      "    aws_conn_id='aws_default',\n",
      "    dag=dag)\n",
      "\n",
      "def analyze_data():\n",
      "    # Code to analyze data\n",
      "    print(\"Data Analysis\")\n",
      "    # Simulated error\n",
      "    raise Exception(\"Simulated error during data analysis\")\n",
      "\n",
      "analyze_data_task = PythonOperator(task_id='analyze_data',\n",
      "                                   python_callable=analyze_data,\n",
      "                                   retries=2,\n",
      "                                   dag=dag)\n",
      "\n",
      "preprocess_task >> create_emr_cluster >> check_emr_cluster >> analyze_data_task\n",
      "\n"
     ]
    }
   ],
   "source": [
    "extracted_metadata = extract_metadata_from_code(code)\n",
    "print('Extracted Metadata:\\n', extracted_metadata)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-24T12:15:17.100446Z",
     "start_time": "2024-01-24T12:14:20.543513Z"
    }
   },
   "id": "497ec5b4b13609b3",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "22ae3670d0e7aa6b"
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
